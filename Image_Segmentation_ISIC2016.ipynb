{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarim711/image_segmentation_ISIC2016/blob/main/Image_Segmentation_ISIC2016.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067625d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "067625d0",
        "outputId": "8c17e225-6027-4aa5-de60-d8a15d28298c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medsegbench\n",
            "  Downloading medsegbench-1.0.0-py3-none-any.whl.metadata (797 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medsegbench) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medsegbench) (1.6.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medsegbench) (0.25.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medsegbench) (11.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medsegbench) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medsegbench) (0.20.1+cu121)\n",
            "Collecting segmentation-models-pytorch (from medsegbench)\n",
            "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsegbench) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsegbench) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsegbench) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsegbench) (2024.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsegbench) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsegbench) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medsegbench) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medsegbench) (3.5.0)\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch->medsegbench)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch->medsegbench) (0.27.0)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch->medsegbench)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch->medsegbench) (1.17.0)\n",
            "Collecting timm==0.9.7 (from segmentation-models-pytorch->medsegbench)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch->medsegbench) (4.67.1)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch->medsegbench)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch->medsegbench) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch->medsegbench) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medsegbench) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medsegbench) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medsegbench) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medsegbench) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->medsegbench) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->medsegbench) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch->medsegbench) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medsegbench) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch->medsegbench) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch->medsegbench) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch->medsegbench) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch->medsegbench) (2024.12.14)\n",
            "Downloading medsegbench-1.0.0-py3-none-any.whl (18 kB)\n",
            "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=0c5a5a1e909af47d9dd496e6d6b30d52a8ea2759457be0502b3d72f139ea952d\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=938cdc345d0f21678f387c652b99032dabd63bbcd61ff6c4f35f486515a4a154\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch, medsegbench\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.12\n",
            "    Uninstalling timm-1.0.12:\n",
            "      Successfully uninstalled timm-1.0.12\n",
            "Successfully installed efficientnet-pytorch-0.7.1 medsegbench-1.0.0 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n"
          ]
        }
      ],
      "source": [
        "# Source: https://github.com/zekikus/MedSegBench\n",
        "\n",
        "# Install medsegbench lib\n",
        "!pip install medsegbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b766e4d8",
      "metadata": {
        "id": "b766e4d8"
      },
      "outputs": [],
      "source": [
        "# import lib and utils from related to the ISIC 2016 dataset (melanoma segmentation)\n",
        "import medsegbench\n",
        "from medsegbench import Isic2016MSBench\n",
        "from medsegbench import INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1655d1",
      "metadata": {
        "id": "2a1655d1"
      },
      "outputs": [],
      "source": [
        "# Get info from the dataset\n",
        "info = INFO[\"isic2016\"]\n",
        "n_channels = info['n_channels_im']\n",
        "n_classes = len(info['pixel_labels'])\n",
        "n_samples = info['n_samples']\n",
        "\n",
        "DataClass = getattr(medsegbench, info['python_class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c716d55a",
      "metadata": {
        "id": "c716d55a"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d822660",
      "metadata": {
        "id": "2d822660"
      },
      "outputs": [],
      "source": [
        "# dataset loading using the original DataClass\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "img_size = 128\n",
        "download = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72495e7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72495e7e",
        "outputId": "682e968f-3bb6-4a7f-8ebb-128a041e69ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medsegbench/isic2016_128.npz\n",
            "Using downloaded and verified file: /root/.medsegbench/isic2016_128.npz\n",
            "Using downloaded and verified file: /root/.medsegbench/isic2016_128.npz\n"
          ]
        }
      ],
      "source": [
        "class AugmentedDataClass(DataClass):\n",
        "    def __init__(self, split, size=128, transform=None, target_transform=None, download=False):\n",
        "        super().__init__(split=split, size=size, transform=transform, target_transform=target_transform, download=download)\n",
        "        self.split = split\n",
        "\n",
        "        # Define augmentations\n",
        "        self.image_augmentations = transforms.Compose([\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Only for image, as segmentation  masks are arrays of class label or binary masks)\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ])\n",
        "\n",
        "        self.mask_augmentations = transforms.Compose([  # Geometric transformations for masks same as image\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = super().__getitem__(idx)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            seed = torch.randint(0, 2**32, (1,)).item()  # Generate a random seed\n",
        "            torch.manual_seed(seed)\n",
        "\n",
        "            # Apply same geometric transformations to both image and mask\n",
        "            image = self.image_augmentations(image)  # Includes color jitter\n",
        "            torch.manual_seed(seed)\n",
        "            mask = self.mask_augmentations(mask)  # Excludes color jitter\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "# Define transformations\n",
        "data_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = AugmentedDataClass(split='train', size=img_size, transform=data_transform, target_transform=data_transform, download=download)\n",
        "validation_dataset = AugmentedDataClass(split='val', size=img_size, transform=data_transform, target_transform=data_transform, download=download)\n",
        "test_dataset = AugmentedDataClass(split='test', size=img_size, transform=data_transform, target_transform=data_transform, download=download)\n",
        "\n",
        "# Dataloader\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset=validation_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608247e0",
      "metadata": {
        "id": "608247e0"
      },
      "outputs": [],
      "source": [
        "# model config\n",
        "NUM_EPOCHS = 10\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b13f02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58b13f02",
        "outputId": "e2da09fe-0c38-44e8-b4eb-1b44bfe136ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output shape: torch.Size([1, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# Unet Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(input_channels, output_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.encoder1 = conv_block(in_channels, 64)\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "        self.encoder4 = conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = conv_block(512, 1024)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.decoder4 = conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoding path\n",
        "        encoder1 = self.encoder1(x)\n",
        "        encoder2 = self.encoder2(self.pool(encoder1))\n",
        "        encoder3 = self.encoder3(self.pool(encoder2))\n",
        "        encoder4 = self.encoder4(self.pool(encoder3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(encoder4))\n",
        "\n",
        "        # Decoding path\n",
        "        decoder4 = self.upconv4(bottleneck)\n",
        "        decoder4 = torch.cat((decoder4, encoder4), dim=1)\n",
        "        decoder4 = self.decoder4(decoder4)\n",
        "\n",
        "        decoder3 = self.upconv3(decoder4)\n",
        "        decoder3 = torch.cat((decoder3, encoder3), dim=1)\n",
        "        decoder3 = self.decoder3(decoder3)\n",
        "\n",
        "        decoder2 = self.upconv2(decoder3)\n",
        "        decoder2 = torch.cat((decoder2, encoder2), dim=1)\n",
        "        decoder2 = self.decoder2(decoder2)\n",
        "\n",
        "        decoder1 = self.upconv1(decoder2)\n",
        "        decoder1 = torch.cat((decoder1, encoder1), dim=1)\n",
        "        decoder1 = self.decoder1(decoder1)\n",
        "\n",
        "        return self.out_conv(decoder1)\n",
        "\n",
        "# Test the model\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "x = torch.randn(1, 3, 256, 256)\n",
        "output = model(x)\n",
        "print(f\"Model output shape: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd47ce56",
      "metadata": {
        "id": "fd47ce56"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c631b38",
      "metadata": {
        "id": "8c631b38"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "KBu0jnjYaQHo"
      },
      "id": "KBu0jnjYaQHo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5186a8ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5186a8ac",
        "outputId": "98181b31-8a9c-487a-802a-726348f9b0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foreground Ratio: 0.2703, Background Ratio: 0.7297\n"
          ]
        }
      ],
      "source": [
        "def calculate_pixel_ratios(dataset):\n",
        "    foreground_pixels = 0\n",
        "    background_pixels = 0\n",
        "\n",
        "    for _, mask in dataset:\n",
        "        mask = mask.squeeze()  # Remove batch dimension if present\n",
        "        foreground_pixels += (mask > 0).sum().item()  # Count positive (foreground) pixels\n",
        "        background_pixels += (mask == 0).sum().item()  # Count zero (background) pixels\n",
        "\n",
        "    total_pixels = foreground_pixels + background_pixels\n",
        "    foreground_ratio = foreground_pixels / total_pixels\n",
        "    background_ratio = background_pixels / total_pixels\n",
        "\n",
        "    return foreground_ratio, background_ratio\n",
        "\n",
        "# Example usage\n",
        "foreground_ratio, background_ratio = calculate_pixel_ratios(train_dataset)\n",
        "print(f\"Foreground Ratio: {foreground_ratio:.4f}, Background Ratio: {background_ratio:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39d5d25",
      "metadata": {
        "id": "c39d5d25"
      },
      "outputs": [],
      "source": [
        "\n",
        "pos_weight_value = background_ratio / foreground_ratio\n",
        "pos_weight = torch.tensor([pos_weight_value],device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ecc170",
      "metadata": {
        "id": "79ecc170"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(preds, targets, threshold=0.5):\n",
        "    # Apply threshold to convert logits to binary predictions\n",
        "    preds = (torch.sigmoid(preds) > threshold).float()\n",
        "    intersection = (preds * targets).sum()  # True positives\n",
        "    union = preds.sum() + targets.sum()  # Total pixels in both masks\n",
        "    dice = (2.0 * intersection) / (union + 1e-8)  # Add epsilon to avoid division by zero\n",
        "    return dice.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c63960",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54c63960",
        "outputId": "f723379c-1642-4dbc-cbad-8852b02ffdda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 0.4438, Train Dice: 0.7712\n",
            "  Val Loss: 0.4138, Val Dice: 0.7598\n",
            "Epoch 2/10\n",
            "  Train Loss: 0.4458, Train Dice: 0.7696\n",
            "  Val Loss: 0.4159, Val Dice: 0.7576\n",
            "Epoch 3/10\n",
            "  Train Loss: 0.4457, Train Dice: 0.7691\n",
            "  Val Loss: 0.4156, Val Dice: 0.7624\n",
            "Epoch 4/10\n",
            "  Train Loss: 0.4407, Train Dice: 0.7677\n",
            "  Val Loss: 0.4181, Val Dice: 0.7453\n",
            "Epoch 5/10\n",
            "  Train Loss: 0.4414, Train Dice: 0.7696\n",
            "  Val Loss: 0.4177, Val Dice: 0.7557\n",
            "Epoch 6/10\n",
            "  Train Loss: 0.4554, Train Dice: 0.7663\n",
            "  Val Loss: 0.4094, Val Dice: 0.7598\n",
            "Epoch 7/10\n",
            "  Train Loss: 0.4413, Train Dice: 0.7713\n",
            "  Val Loss: 0.4138, Val Dice: 0.7626\n",
            "Epoch 8/10\n",
            "  Train Loss: 0.4332, Train Dice: 0.7721\n",
            "  Val Loss: 0.4180, Val Dice: 0.7591\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.4368, Train Dice: 0.7737\n",
            "  Val Loss: 0.4147, Val Dice: 0.7642\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.4319, Train Dice: 0.7737\n",
            "  Val Loss: 0.4200, Val Dice: 0.7588\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = []\n",
        "        train_dice_scores = []\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            # Backward pass + optimization step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate Dice coefficient for training\n",
        "            dice = dice_coefficient(outputs, targets)\n",
        "            train_dice_scores.append(dice)\n",
        "\n",
        "        # Compute average training metrics\n",
        "        avg_train_loss = sum(train_loss) / len(train_loss)\n",
        "        avg_train_dice = sum(train_dice_scores) / len(train_dice_scores)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss = []\n",
        "        val_dice_scores = []\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # Disable gradient computation for validation\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss.append(loss.item())\n",
        "\n",
        "                # Calculate Dice coefficient for validation\n",
        "                dice = dice_coefficient(outputs, targets)\n",
        "                val_dice_scores.append(dice)\n",
        "\n",
        "        # Compute average validation metrics\n",
        "        avg_val_loss = sum(val_loss) / len(val_loss)\n",
        "        avg_val_dice = sum(val_dice_scores) / len(val_dice_scores)\n",
        "\n",
        "        # Print metrics for the epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Dice: {avg_train_dice:.4f}\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Dice: {avg_val_dice:.4f}\")\n",
        "\n",
        "train_model(model=model, train_loader=train_loader, val_loader=val_loader, criterion=criterion, optimizer=optimizer, num_epochs=NUM_EPOCHS, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6e21db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6e21db",
        "outputId": "4dc8a1b5-1242-4d68-8ab9-cc4e576008de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "import torch.nn.functional as F # import functional interface\n",
        "class UNetWithResNetEncoder(nn.Module):\n",
        "    def __init__(self, out_channels=1):\n",
        "        super(UNetWithResNetEncoder, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet backbone\n",
        "        resnet = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
        "        self.encoder1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)  # First layer\n",
        "        self.encoder2 = resnet.layer1  # ResNet block 1\n",
        "        self.encoder3 = resnet.layer2  # ResNet block 2\n",
        "        self.encoder4 = resnet.layer3  # ResNet block 3\n",
        "\n",
        "        # Modify the last ResNet block (layer4) to prevent excessive downsampling\n",
        "        self.encoder5 = resnet.layer4\n",
        "        for name, layer in self.encoder5.named_modules():\n",
        "            if isinstance(layer, nn.Conv2d) and layer.stride == (2, 2):\n",
        "                layer.stride = (1, 1)  # Reduce downsampling\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.decoder4 = self.conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3 = self.conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2 = self.conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = self.conv_block(128, 64)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoding path\n",
        "        encoder1 = self.encoder1(x)\n",
        "        encoder2 = self.encoder2(F.max_pool2d(encoder1, 2))\n",
        "        encoder3 = self.encoder3(F.max_pool2d(encoder2, 2))\n",
        "        encoder4 = self.encoder4(F.max_pool2d(encoder3, 2))\n",
        "        encoder5 = self.encoder5(F.max_pool2d(encoder4, 2))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(encoder5)\n",
        "\n",
        "        # Decoding path\n",
        "        decoder4 = self.upconv4(bottleneck)\n",
        "        decoder4 = torch.cat((decoder4, F.interpolate(encoder5, size=decoder4.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder4 = self.decoder4(decoder4)\n",
        "\n",
        "        decoder3 = self.upconv3(decoder4)\n",
        "        decoder3 = torch.cat((decoder3, F.interpolate(encoder4, size=decoder3.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder3 = self.decoder3(decoder3)\n",
        "\n",
        "        decoder2 = self.upconv2(decoder3)\n",
        "        decoder2 = torch.cat((decoder2, F.interpolate(encoder3, size=decoder2.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder2 = self.decoder2(decoder2)\n",
        "\n",
        "        decoder1 = self.upconv1(decoder2)\n",
        "        decoder1 = torch.cat((decoder1, F.interpolate(encoder2, size=decoder1.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder1 = self.decoder1(decoder1)\n",
        "\n",
        "        # Final upsampling to match input size\n",
        "        final_output = F.interpolate(decoder1, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return self.out_conv(final_output)\n",
        "\n",
        "\n",
        "# Instantiate and test the updated model\n",
        "model_resnet_unet = UNetWithResNetEncoder(out_channels=1)\n",
        "x = torch.randn(1, 3, 256, 256)\n",
        "output = model_resnet_unet(x)\n",
        "print(f\"Output shape: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776e7abb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "776e7abb",
        "outputId": "77b52cd3-6895-4267-8cf5-86b2c22a7baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 0.5709, Train Dice: 0.7154\n",
            "  Val Loss: 0.5911, Val Dice: 0.6221\n",
            "Epoch 2/10\n",
            "  Train Loss: 0.4539, Train Dice: 0.7731\n",
            "  Val Loss: 0.3926, Val Dice: 0.7763\n",
            "Epoch 3/10\n",
            "  Train Loss: 0.4306, Train Dice: 0.7715\n",
            "  Val Loss: 0.3492, Val Dice: 0.7777\n",
            "Epoch 4/10\n",
            "  Train Loss: 0.3933, Train Dice: 0.7935\n",
            "  Val Loss: 0.3183, Val Dice: 0.8072\n",
            "Epoch 5/10\n",
            "  Train Loss: 0.3994, Train Dice: 0.7966\n",
            "  Val Loss: 0.3340, Val Dice: 0.8152\n",
            "Epoch 6/10\n",
            "  Train Loss: 0.3917, Train Dice: 0.7916\n",
            "  Val Loss: 0.4314, Val Dice: 0.7198\n",
            "Epoch 7/10\n",
            "  Train Loss: 0.3826, Train Dice: 0.7991\n",
            "  Val Loss: 0.3219, Val Dice: 0.8401\n",
            "Epoch 8/10\n",
            "  Train Loss: 0.3770, Train Dice: 0.7954\n",
            "  Val Loss: 0.3620, Val Dice: 0.7675\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.3799, Train Dice: 0.8010\n",
            "  Val Loss: 0.3484, Val Dice: 0.7769\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.3594, Train Dice: 0.8077\n",
            "  Val Loss: 0.3835, Val Dice: 0.7544\n"
          ]
        }
      ],
      "source": [
        "# Train the model with training and validation\n",
        "NUM_EPOCHS = 10\n",
        "lr = 0.001\n",
        "\n",
        "# Initialize model, criterion, optimizer, and dataloaders\n",
        "model_resnet_unet = UNetWithResNetEncoder(out_channels=1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model_resnet_unet.parameters(), lr=lr)\n",
        "\n",
        "train_model(model=model_resnet_unet, train_loader=train_loader, val_loader=val_loader, criterion=criterion, optimizer=optimizer, num_epochs=NUM_EPOCHS, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2974d52a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2974d52a",
        "outputId": "5ede33f0-2505-4e4c-c114-71b00e7876ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Dice Coefficient on Test Dataset: 0.7797\n"
          ]
        }
      ],
      "source": [
        "def predict(model, image, device, threshold=0.5):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)  # Move the image to the specified device\n",
        "        output = model(image)  # Forward pass through the model\n",
        "        output = torch.sigmoid(output)  # Apply sigmoid to get probabilities\n",
        "        mask = (output > threshold).float()  # Binarize using the threshold\n",
        "    return mask.squeeze(0).squeeze(0)  # Remove batch and channel dimensions\n",
        "\n",
        "def evaluate_on_test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    dice_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_loader:\n",
        "            images, masks = images.to(device), masks.to(device)  # Move data to the device\n",
        "\n",
        "            # Make predictions\n",
        "            outputs = model(images)\n",
        "            outputs = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
        "            preds = (outputs > 0.5).float()  # Binarize predictions\n",
        "\n",
        "            # Compute Dice coefficient\n",
        "            dice = dice_coefficient(preds, masks)\n",
        "            dice_scores.append(dice)\n",
        "\n",
        "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
        "    print(f\"Average Dice Coefficient on Test Dataset: {avg_dice:.4f}\")\n",
        "\n",
        "evaluate_on_test(model_resnet_unet, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}