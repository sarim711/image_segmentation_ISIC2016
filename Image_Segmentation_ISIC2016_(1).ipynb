{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarim711/image_segmentation_ISIC2016/blob/main/Image_Segmentation_ISIC2016_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067625d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "067625d0",
        "outputId": "02abd3c2-5165-4a4f-de77-9bf77f197cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medsegbench\n",
            "  Downloading medsegbench-1.0.0-py3-none-any.whl.metadata (797 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medsegbench) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medsegbench) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medsegbench) (0.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medsegbench) (11.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medsegbench) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medsegbench) (0.21.0+cu124)\n",
            "Collecting segmentation-models-pytorch (from medsegbench)\n",
            "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medsegbench) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medsegbench) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medsegbench) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medsegbench) (2025.2.18)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medsegbench) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medsegbench) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medsegbench) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medsegbench) (3.6.0)\n",
            "Collecting efficientnet-pytorch>=0.6.1 (from segmentation-models-pytorch->medsegbench)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch->medsegbench) (0.28.1)\n",
            "Collecting pretrainedmodels>=0.7.1 (from segmentation-models-pytorch->medsegbench)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch->medsegbench) (1.17.0)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch->medsegbench) (1.0.15)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch->medsegbench) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->medsegbench)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->medsegbench)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->medsegbench)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->medsegbench)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->medsegbench)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->medsegbench)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->medsegbench)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->medsegbench)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->medsegbench)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->medsegbench)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medsegbench) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medsegbench) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->medsegbench) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->medsegbench) (2.32.3)\n",
            "Collecting munch (from pretrainedmodels>=0.7.1->segmentation-models-pytorch->medsegbench)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.9->segmentation-models-pytorch->medsegbench) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medsegbench) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch->medsegbench) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch->medsegbench) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch->medsegbench) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch->medsegbench) (2025.1.31)\n",
            "Downloading medsegbench-1.0.0-py3-none-any.whl (18 kB)\n",
            "Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=e19a376fab7cdf6928f91ff1161c6f2c9e1edf8d13bf37fe0ddd8340dcc2c1ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=1dbeadf937e7e484e2e79b6cccf885acfa024044b36a5837c770d19cc4c46b8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet-pytorch, pretrainedmodels, segmentation-models-pytorch, medsegbench\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed efficientnet-pytorch-0.7.1 medsegbench-1.0.0 munch-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.4.0\n"
          ]
        }
      ],
      "source": [
        "# Source: https://github.com/zekikus/MedSegBench\n",
        "\n",
        "# Install medsegbench lib\n",
        "!pip install medsegbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b766e4d8",
      "metadata": {
        "id": "b766e4d8"
      },
      "outputs": [],
      "source": [
        "# import lib and utils from related to the ISIC 2016 dataset (melanoma segmentation)\n",
        "import medsegbench\n",
        "from medsegbench import Isic2016MSBench\n",
        "from medsegbench import INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1655d1",
      "metadata": {
        "id": "2a1655d1"
      },
      "outputs": [],
      "source": [
        "# Get info from the dataset\n",
        "info = INFO[\"isic2016\"]\n",
        "n_channels = info['n_channels_im']\n",
        "n_classes = len(info['pixel_labels'])\n",
        "n_samples = info['n_samples']\n",
        "\n",
        "DataClass = getattr(medsegbench, info['python_class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c716d55a",
      "metadata": {
        "id": "c716d55a"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d822660",
      "metadata": {
        "id": "2d822660"
      },
      "outputs": [],
      "source": [
        "# dataset loading using the original DataClass\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "img_size = 128\n",
        "download = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72495e7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72495e7e",
        "outputId": "2011af0b-b475-49e9-ca7a-1112c547e8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47.9M/47.9M [00:36<00:00, 1.31MB/s]\n"
          ]
        }
      ],
      "source": [
        "class AugmentedDataClass(DataClass):\n",
        "    def __init__(self, split, size=128, transform=None, target_transform=None, download=False):\n",
        "        super().__init__(split=split, size=size, transform=transform, target_transform=target_transform, download=download)\n",
        "        self.split = split\n",
        "\n",
        "        # Define augmentations\n",
        "        self.image_augmentations = transforms.Compose([\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Only for image, as segmentation  masks are arrays of class label or binary masks)\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ])\n",
        "\n",
        "        self.mask_augmentations = transforms.Compose([  # Geometric transformations for masks same as image\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = super().__getitem__(idx)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            seed = torch.randint(0, 2**32, (1,)).item()  # Generate a random seed\n",
        "            torch.manual_seed(seed)\n",
        "\n",
        "            # Apply same geometric transformations to both image and mask\n",
        "            image = self.image_augmentations(image)  # Includes color jitter\n",
        "            torch.manual_seed(seed)\n",
        "            mask = self.mask_augmentations(mask)  # Excludes color jitter\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "# Define transformations\n",
        "data_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = AugmentedDataClass(split='train', size=img_size, transform=data_transform, target_transform=data_transform, download=download)\n",
        "validation_dataset = AugmentedDataClass(split='val', size=img_size, transform=data_transform, target_transform=data_transform, download=download)\n",
        "test_dataset = AugmentedDataClass(split='test', size=img_size, transform=data_transform, target_transform=data_transform, download=download)\n",
        "\n",
        "# Dataloader\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset=validation_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608247e0",
      "metadata": {
        "id": "608247e0"
      },
      "outputs": [],
      "source": [
        "# model config\n",
        "NUM_EPOCHS = 10\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b13f02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58b13f02",
        "outputId": "d30b76e2-9652-40c0-b625-55afd6a331e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output shape: torch.Size([1, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# Unet Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(input_channels, output_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.encoder1 = conv_block(in_channels, 64)\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "        self.encoder4 = conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = conv_block(512, 1024)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.decoder4 = conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoding path\n",
        "        encoder1 = self.encoder1(x)\n",
        "        encoder2 = self.encoder2(self.pool(encoder1))\n",
        "        encoder3 = self.encoder3(self.pool(encoder2))\n",
        "        encoder4 = self.encoder4(self.pool(encoder3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(encoder4))\n",
        "\n",
        "        # Decoding path\n",
        "        decoder4 = self.upconv4(bottleneck)\n",
        "        decoder4 = torch.cat((decoder4, encoder4), dim=1)\n",
        "        decoder4 = self.decoder4(decoder4)\n",
        "\n",
        "        decoder3 = self.upconv3(decoder4)\n",
        "        decoder3 = torch.cat((decoder3, encoder3), dim=1)\n",
        "        decoder3 = self.decoder3(decoder3)\n",
        "\n",
        "        decoder2 = self.upconv2(decoder3)\n",
        "        decoder2 = torch.cat((decoder2, encoder2), dim=1)\n",
        "        decoder2 = self.decoder2(decoder2)\n",
        "\n",
        "        decoder1 = self.upconv1(decoder2)\n",
        "        decoder1 = torch.cat((decoder1, encoder1), dim=1)\n",
        "        decoder1 = self.decoder1(decoder1)\n",
        "\n",
        "        return self.out_conv(decoder1)\n",
        "\n",
        "# Test the model\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "x = torch.randn(1, 3, 256, 256)\n",
        "output = model(x)\n",
        "print(f\"Model output shape: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd47ce56",
      "metadata": {
        "id": "fd47ce56"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c631b38",
      "metadata": {
        "id": "8c631b38"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "KBu0jnjYaQHo"
      },
      "id": "KBu0jnjYaQHo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5186a8ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5186a8ac",
        "outputId": "14293a89-16b8-4928-edc1-b92a63f2cd23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foreground Ratio: 0.2704, Background Ratio: 0.7296\n"
          ]
        }
      ],
      "source": [
        "def calculate_pixel_ratios(dataset):\n",
        "    foreground_pixels = 0\n",
        "    background_pixels = 0\n",
        "\n",
        "    for _, mask in dataset:\n",
        "        mask = mask.squeeze()  # Remove batch dimension if present\n",
        "        foreground_pixels += (mask > 0).sum().item()  # Count positive (foreground) pixels\n",
        "        background_pixels += (mask == 0).sum().item()  # Count zero (background) pixels\n",
        "\n",
        "    total_pixels = foreground_pixels + background_pixels\n",
        "    foreground_ratio = foreground_pixels / total_pixels\n",
        "    background_ratio = background_pixels / total_pixels\n",
        "\n",
        "    return foreground_ratio, background_ratio\n",
        "\n",
        "# Example usage\n",
        "foreground_ratio, background_ratio = calculate_pixel_ratios(train_dataset)\n",
        "print(f\"Foreground Ratio: {foreground_ratio:.4f}, Background Ratio: {background_ratio:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39d5d25",
      "metadata": {
        "id": "c39d5d25"
      },
      "outputs": [],
      "source": [
        "\n",
        "pos_weight_value = background_ratio / foreground_ratio\n",
        "pos_weight = torch.tensor([pos_weight_value],device=device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ecc170",
      "metadata": {
        "id": "79ecc170"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(preds, targets, threshold=0.5):\n",
        "    # Apply threshold to convert logits to binary predictions\n",
        "    preds = (torch.sigmoid(preds) > threshold).float()\n",
        "    intersection = (preds * targets).sum()  # True positives\n",
        "    union = preds.sum() + targets.sum()  # Total pixels in both masks\n",
        "    dice = (2.0 * intersection) / (union + 1e-8)  # Add epsilon to avoid division by zero\n",
        "    return dice.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c63960",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54c63960",
        "outputId": "d2063fb7-3610-4ef4-f230-6d2f80a1e2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 0.7402, Train Dice: 0.6312\n",
            "  Val Loss: 4.9304, Val Dice: 0.4849\n",
            "Epoch 2/10\n",
            "  Train Loss: 0.6357, Train Dice: 0.6862\n",
            "  Val Loss: 0.8690, Val Dice: 0.5243\n",
            "Epoch 3/10\n",
            "  Train Loss: 0.6182, Train Dice: 0.6951\n",
            "  Val Loss: 0.6470, Val Dice: 0.6379\n",
            "Epoch 4/10\n",
            "  Train Loss: 0.5919, Train Dice: 0.7104\n",
            "  Val Loss: 0.4823, Val Dice: 0.7419\n",
            "Epoch 5/10\n",
            "  Train Loss: 0.5795, Train Dice: 0.7193\n",
            "  Val Loss: 0.5069, Val Dice: 0.7308\n",
            "Epoch 6/10\n",
            "  Train Loss: 0.5475, Train Dice: 0.7331\n",
            "  Val Loss: 0.7931, Val Dice: 0.5288\n",
            "Epoch 7/10\n",
            "  Train Loss: 0.5502, Train Dice: 0.7281\n",
            "  Val Loss: 0.6690, Val Dice: 0.6354\n",
            "Epoch 8/10\n",
            "  Train Loss: 0.5457, Train Dice: 0.7274\n",
            "  Val Loss: 0.4763, Val Dice: 0.7218\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.5107, Train Dice: 0.7420\n",
            "  Val Loss: 0.4753, Val Dice: 0.7919\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.5031, Train Dice: 0.7527\n",
            "  Val Loss: 0.4807, Val Dice: 0.7269\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = []\n",
        "        train_dice_scores = []\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            # Backward pass + optimization step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate Dice coefficient for training\n",
        "            dice = dice_coefficient(outputs, targets)\n",
        "            train_dice_scores.append(dice)\n",
        "\n",
        "        # Compute average training metrics\n",
        "        avg_train_loss = sum(train_loss) / len(train_loss)\n",
        "        avg_train_dice = sum(train_dice_scores) / len(train_dice_scores)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss = []\n",
        "        val_dice_scores = []\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # Disable gradient computation for validation\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss.append(loss.item())\n",
        "\n",
        "                # Calculate Dice coefficient for validation\n",
        "                dice = dice_coefficient(outputs, targets)\n",
        "                val_dice_scores.append(dice)\n",
        "\n",
        "        # Compute average validation metrics\n",
        "        avg_val_loss = sum(val_loss) / len(val_loss)\n",
        "        avg_val_dice = sum(val_dice_scores) / len(val_dice_scores)\n",
        "\n",
        "        # Print metrics for the epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Dice: {avg_train_dice:.4f}\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Dice: {avg_val_dice:.4f}\")\n",
        "\n",
        "train_model(model=model, train_loader=train_loader, val_loader=val_loader, criterion=criterion, optimizer=optimizer, num_epochs=NUM_EPOCHS, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_on_test(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4DY8WAWZyi9",
        "outputId": "aa38b944-e288-4bf6-c0f1-52dcadc58012"
      },
      "id": "j4DY8WAWZyi9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Dice Coefficient on Test Dataset: 0.7502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6e21db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6e21db",
        "outputId": "007a4e6d-caaa-4645-cf05-745dffb0e0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 166MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "import torch.nn.functional as F # import functional interface\n",
        "class UNetWithResNetEncoder(nn.Module):\n",
        "    def __init__(self, out_channels=1):\n",
        "        super(UNetWithResNetEncoder, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet backbone\n",
        "        resnet = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
        "        self.encoder1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)  # First layer\n",
        "        self.encoder2 = resnet.layer1  # ResNet block 1\n",
        "        self.encoder3 = resnet.layer2  # ResNet block 2\n",
        "        self.encoder4 = resnet.layer3  # ResNet block 3\n",
        "\n",
        "        # Modify the last ResNet block (layer4) to prevent excessive downsampling\n",
        "        self.encoder5 = resnet.layer4\n",
        "        for name, layer in self.encoder5.named_modules():\n",
        "            if isinstance(layer, nn.Conv2d) and layer.stride == (2, 2):\n",
        "                layer.stride = (1, 1)  # Reduce downsampling\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.decoder4 = self.conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3 = self.conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2 = self.conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = self.conv_block(128, 64)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoding path\n",
        "        encoder1 = self.encoder1(x)\n",
        "        encoder2 = self.encoder2(F.max_pool2d(encoder1, 2))\n",
        "        encoder3 = self.encoder3(F.max_pool2d(encoder2, 2))\n",
        "        encoder4 = self.encoder4(F.max_pool2d(encoder3, 2))\n",
        "        encoder5 = self.encoder5(F.max_pool2d(encoder4, 2))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(encoder5)\n",
        "\n",
        "        # Decoding path\n",
        "        decoder4 = self.upconv4(bottleneck)\n",
        "        decoder4 = torch.cat((decoder4, F.interpolate(encoder5, size=decoder4.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder4 = self.decoder4(decoder4)\n",
        "\n",
        "        decoder3 = self.upconv3(decoder4)\n",
        "        decoder3 = torch.cat((decoder3, F.interpolate(encoder4, size=decoder3.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder3 = self.decoder3(decoder3)\n",
        "\n",
        "        decoder2 = self.upconv2(decoder3)\n",
        "        decoder2 = torch.cat((decoder2, F.interpolate(encoder3, size=decoder2.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder2 = self.decoder2(decoder2)\n",
        "\n",
        "        decoder1 = self.upconv1(decoder2)\n",
        "        decoder1 = torch.cat((decoder1, F.interpolate(encoder2, size=decoder1.shape[2:], mode=\"bilinear\", align_corners=False)), dim=1)\n",
        "        decoder1 = self.decoder1(decoder1)\n",
        "\n",
        "        # Final upsampling to match input size\n",
        "        final_output = F.interpolate(decoder1, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return self.out_conv(final_output)\n",
        "\n",
        "\n",
        "# Instantiate and test the updated model\n",
        "model_resnet_unet = UNetWithResNetEncoder(out_channels=1)\n",
        "x = torch.randn(1, 3, 256, 256)\n",
        "output = model_resnet_unet(x)\n",
        "print(f\"Output shape: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776e7abb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "776e7abb",
        "outputId": "874511a9-2cdc-4823-8ea3-f55845d3b17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 0.5677, Train Dice: 0.7259\n",
            "  Val Loss: 2.1767, Val Dice: 0.5009\n",
            "Epoch 2/10\n",
            "  Train Loss: 0.4407, Train Dice: 0.7863\n",
            "  Val Loss: 0.4156, Val Dice: 0.7589\n",
            "Epoch 3/10\n",
            "  Train Loss: 0.4271, Train Dice: 0.7733\n",
            "  Val Loss: 0.5171, Val Dice: 0.6980\n",
            "Epoch 4/10\n",
            "  Train Loss: 0.4015, Train Dice: 0.7917\n",
            "  Val Loss: 0.3535, Val Dice: 0.7859\n",
            "Epoch 5/10\n",
            "  Train Loss: 0.4088, Train Dice: 0.7925\n",
            "  Val Loss: 0.3706, Val Dice: 0.7809\n",
            "Epoch 6/10\n",
            "  Train Loss: 0.3959, Train Dice: 0.7961\n",
            "  Val Loss: 0.4648, Val Dice: 0.7046\n",
            "Epoch 7/10\n",
            "  Train Loss: 0.3812, Train Dice: 0.7992\n",
            "  Val Loss: 0.3775, Val Dice: 0.7584\n",
            "Epoch 8/10\n",
            "  Train Loss: 0.3732, Train Dice: 0.8043\n",
            "  Val Loss: 0.4070, Val Dice: 0.7783\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.3639, Train Dice: 0.8052\n",
            "  Val Loss: 0.4476, Val Dice: 0.7096\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.3557, Train Dice: 0.8085\n",
            "  Val Loss: 0.2857, Val Dice: 0.8392\n"
          ]
        }
      ],
      "source": [
        "# Train the model with training and validation\n",
        "NUM_EPOCHS = 10\n",
        "lr = 0.001\n",
        "\n",
        "# Initialize model, criterion, optimizer, and dataloaders\n",
        "model_resnet_unet = UNetWithResNetEncoder(out_channels=1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model_resnet_unet.parameters(), lr=lr)\n",
        "\n",
        "train_model(model=model_resnet_unet, train_loader=train_loader, val_loader=val_loader, criterion=criterion, optimizer=optimizer, num_epochs=NUM_EPOCHS, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2974d52a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2974d52a",
        "outputId": "3337af48-f798-417b-e4bb-86fcfcc8c95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Dice Coefficient on Test Dataset: 0.8499\n"
          ]
        }
      ],
      "source": [
        "def predict(model, image, device, threshold=0.5):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)  # Move the image to the specified device\n",
        "        output = model(image)  # Forward pass through the model\n",
        "        output = torch.sigmoid(output)  # Apply sigmoid to get probabilities\n",
        "        mask = (output > threshold).float()  # Binarize using the threshold\n",
        "    return mask.squeeze(0).squeeze(0)  # Remove batch and channel dimensions\n",
        "\n",
        "def evaluate_on_test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    dice_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_loader:\n",
        "            images, masks = images.to(device), masks.to(device)  # Move data to the device\n",
        "\n",
        "            # Make predictions\n",
        "            outputs = model(images)\n",
        "            outputs = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
        "            preds = (outputs > 0.5).float()  # Binarize predictions\n",
        "\n",
        "            # Compute Dice coefficient\n",
        "            dice = dice_coefficient(preds, masks)\n",
        "            dice_scores.append(dice)\n",
        "\n",
        "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
        "    print(f\"Average Dice Coefficient on Test Dataset: {avg_dice:.4f}\")\n",
        "\n",
        "evaluate_on_test(model_resnet_unet, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}